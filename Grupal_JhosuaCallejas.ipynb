{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa344af",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"font-family: 'Times New Roman', sans-serif; color: darkblue\">Inteligencia Artificial</h1>\n",
    "<h2 align=center style=\"font-family: 'Times New Roman', sans-serif; color: darkblue\">Practica Grupal</h2>\n",
    "<h3 align=center style=\"font-family: 'Times New Roman', sans-serif; color: darkblue\">Jhosua Callejas Ramos</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6a9a0",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Importacion de librerias </p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "465a419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import glob\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "055a840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd66564",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Funcion para leer y procesar las imágenes de manera más eficiente y usamos tqdm para monitorear el progreso</p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0c426",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Especificamos los directorios de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa683a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'food-101'\n",
    "path = os.path.join(base_path, 'images')\n",
    "meta_path = os.path.join(base_path, 'meta')\n",
    "\n",
    "train_path = os.path.join(meta_path, 'train.txt')  \n",
    "test_path = os.path.join(meta_path, 'test.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97299d4",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Cargamos y redimensionamos los datos de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22c75455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataBunch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\3786648922.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m data = ImageDataBunch.from_folder(path, valid_pct=0.2,\n\u001b[0m\u001b[0;32m      2\u001b[0m         ds_tfms=get_transforms(), size=224, num_workers=8, bs=64).normalize(imagenet_stats)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageDataBunch' is not defined"
     ]
    }
   ],
   "source": [
    "data = ImageDataBunch.from_folder(path, valid_pct=0.2,\n",
    "        ds_tfms=get_transforms(), size=224, num_workers=8, bs=64).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0f34a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fastai.vision.data' has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\2130496328.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'fastai.vision.data' has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1a0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67a31dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba869915",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfmCrop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\1856607239.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# defining the data augmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m ds_tfms = [RandTransform(tfm=TfmCrop(crop_pad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1), 'padding_mode': 'reflection'}, p=1.0, resolved={}),\n\u001b[0m\u001b[0;32m      5\u001b[0m            \u001b[0mRandTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfmCoord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymmetric_warp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'magnitude'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_random\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m            \u001b[0mRandTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfmAffine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'degrees'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_random\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfmCrop' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "# defining the data augmentation\n",
    "ds_tfms = [RandTransform(tfm=TfmCrop(crop_pad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1), 'padding_mode': 'reflection'}, p=1.0, resolved={}),\n",
    "           RandTransform(tfm=TfmCoord(symmetric_warp), kwargs={'magnitude': (-0.2, 0.2)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmAffine(rotate), kwargs={'degrees': (-40, 40)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmAffine(flip_affine), kwargs={}, p=0.5, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmAffine(zoom), kwargs={'scale': (1.0, 1.4), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmLighting(brightness), kwargs={'change': (0.35, 0.65)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmLighting(contrast), kwargs={'scale': (0.7, 1.43)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmCoord(jitter), kwargs={'magnitude': (-0.01, 0.01)}, p=0.3, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmCoord(skew), kwargs={'direction': (0, 7), 'magnitude': (0.2)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmAffine(squish), kwargs={'scale': (0.42, 2.4), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "           RandTransform(tfm=TfmCrop(crop_pad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1)}, p=1.0, resolved={}, do_run=True, is_random=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d094d",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Convertimos las listas en arrays de NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59644af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\3844275104.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m data = (ImageList.from_df(df=train_df, path=path/'images', cols=1)\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0msplit_by_rand_pct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mlabel_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_tfm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageList' is not defined"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "data = (ImageList.from_df(df=train_df, path=path/'images', cols=1)\n",
    "        .split_by_rand_pct(0.2)\n",
    "        .label_from_df(cols=0)\n",
    "        .transform(ds_tfm, size=img_size)\n",
    "        .databuch(bs=bs)\n",
    "        .normalize(imagenet_stats)\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d15c74",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Verificar las dimensiones y tamaños de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bbed8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet-pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from efficientnet-pytorch) (2.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (3.2.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (2021.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (4.12.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (2024.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (3.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (3.15.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (1.12.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->efficientnet-pytorch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->efficientnet-pytorch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from jinja2->torch->efficientnet-pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\jhosu\\anaconda3\\lib\\site-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py): started\n",
      "  Building wheel for efficientnet-pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=aef4ae3aa56be8808ce2ae8ff7927b6f23acec152707ea9ee0e23b3506ec978c\n",
      "  Stored in directory: c:\\users\\jhosu\\appdata\\local\\pip\\cache\\wheels\\29\\16\\24\\752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b0e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to C:\\Users\\JHOSU/.cache\\torch\\hub\\checkpoints\\efficientnet-b0-355c32eb.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 20.4M/20.4M [00:00<00:00, 40.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ca068ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ShowGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\1978094995.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtop_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_k_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback_fns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mShowGraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_fp16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_head\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ShowGraph' is not defined"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from functools import partial\n",
    "\n",
    "top_5 = partial(top_k_accuracy, k=5)\n",
    "\n",
    "learn = Learner(data, model, metrics=[accuracy, top_5], callback_fns=ShowGraph).to_fp16()\n",
    "learn.split(lambda m: (model._conv_head,))\n",
    "\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08d1c5",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Normalizamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c875eb18",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36892\\2020354787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Normalizar los valores de los píxeles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_images\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "# Normalizar los valores de los píxeles\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdcb5a6",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Convertir las etiquetas a valores numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21112f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "train_labels_one_hot = keras.utils.to_categorical(train_labels_encoded, num_classes=len(label_encoder.classes_))\n",
    "test_labels_one_hot = keras.utils.to_categorical(test_labels_encoded, num_classes=len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feef452",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Definimos y compilar el modelo</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo VGG16 preentrenado sin las capas superiores\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congelar todas las capas en el modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Añadir nuevas capas\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
    "\n",
    "# Crear el modelo completo\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4f98a",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Entrenamos el modelo</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'),\n",
    "        keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1f50e",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Evaluamos el modelo</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0270e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_labels_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87469ea",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: darkblue\">Guardamos el modelo</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('food101_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
